{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Learning on the MNIST Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Contrastive Transformations\n",
    "- RandomHorizontalFlip()\n",
    "- RandomResizedCrop(size=28, scale=(0.8, 1.0)) \n",
    "- RandomApply([ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8), \n",
    "- RandomGrayscale(p=0.2), \n",
    "- GaussianBlur(kernel_size=9), \n",
    "- ToTensor(), \n",
    "- Normalize((0.1307,), (0.3081,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Pretraining Batch Size\n",
    "\n",
    "- pretraining_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "- conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "- fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "- fc2 = nn.Linear(500, 10)\n",
    "\n",
    "CNN Base Encoder:\n",
    "- x = torch.relu(self.conv1(x))  # 1x28x28 -> 20x24x24\n",
    "- x = torch.max_pool2d(x, 2, 2)  # 20x24x24 -> 20x12x12\n",
    "- x = torch.relu(self.conv2(x))  # 20x12x12 -> 50x8x8\n",
    "- x = torch.max_pool2d(x, 2, 2)  # 50x8x8 -> 50x4x4\n",
    "- x = x.view(-1, 4 * 4 * 50)     # 50x4x4 -> 800\n",
    "\n",
    "Projection Head:\n",
    "- x = torch.relu(self.fc1(x))    # 800 -> 500\n",
    "- x = self.fc2(x)                # 500 -> 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os, random, copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomCrop, RandomResizedCrop, ColorJitter, GaussianBlur, RandomApply, RandomGrayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Setting the Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 16):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Preparing the MNIST Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Defining Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "contrastive_transform = Compose(\n",
    "    [RandomHorizontalFlip(), \n",
    "     RandomResizedCrop(size=28, scale=(0.8, 1.0)), \n",
    "     RandomApply([ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8), \n",
    "     RandomGrayscale(p=0.2), \n",
    "     GaussianBlur(kernel_size=9), \n",
    "     ToTensor(), \n",
    "     Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "class CLTransformations(object):\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        xi = self.transform(sample)\n",
    "        xj = self.transform(sample)\n",
    "        return xi, xj\n",
    "    \n",
    "contrastive_transform = CLTransformations(contrastive_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why those normalisations?\n",
    "- create a table with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Defining Dataset Size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=contrastive_transform)\n",
    "trainset = torch.utils.data.Subset(trainset, range(dataset_size))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=pretraining_batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=contrastive_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=pretraining_batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three', 'four',\n",
    "           'five', 'six', 'seven', 'eight', 'nine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Visualising the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.28789353..1.9107434].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four  eight nine  two  \n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "show_batch_size = 4\n",
    "show_trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "show_trainloader = torch.utils.data.DataLoader(show_trainset, batch_size=show_batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "show_testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "show_testloader = torch.utils.data.DataLoader(show_testset, batch_size=show_batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three', 'four',\n",
    "           'five', 'six', 'seven', 'eight', 'nine')\n",
    "\n",
    "# getting random training images\n",
    "dataiter = iter(show_trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(show_batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what does imshow do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Defining and Initialising the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Base Encoder and Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.layers = nn.Sequential(\n",
    "            self.fc1, nn.ReLU(),        # 800 -> 500\n",
    "            self.fc2)                   # 500 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class CLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.projection_head = ProjectionHead(800, 500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN Base Encoder\n",
    "        x = torch.relu(self.conv1(x))  \n",
    "        x = torch.max_pool2d(x, 2, 2)  # 20x24x24 -> 20x12x12\n",
    "        x = torch.relu(self.conv2(x))  # 20x12x12 -> 50x8x8\n",
    "        x = torch.max_pool2d(x, 2, 2)  # 50x8x8 -> 50x4x4\n",
    "        x = x.view(-1, 4 * 4 * 50)     # 50x4x4 -> 800\n",
    "\n",
    "        # Projection Head\n",
    "        x = self.projection_head(x)                \n",
    "        return x\n",
    "    \n",
    "    def forward_no_projection_head(self, x):\n",
    "        # CNN Base Encoder\n",
    "        x = torch.relu(self.conv1(x))  \n",
    "        x = torch.max_pool2d(x, 2, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(hi, hj, temperature=0.5):\n",
    "    # concatenate two sets of features\n",
    "    h = torch.cat([hi, hj], dim=0)\n",
    "\n",
    "    # calculate cosine similarity\n",
    "    cos_sim = F.cosine_similarity(h.unsqueeze(1), h.unsqueeze(0), dim=2)\n",
    "\n",
    "    # remove self-similarity form the matrix\n",
    "    cos_sim = cos_sim.masked_fill(torch.eye(cos_sim.shape[0]).bool(), -1e9)\n",
    "\n",
    "    # for each image, find the positive pair\n",
    "    positive_pairs = torch.cat([torch.arange(hi.shape[0], 2*hi.shape[0]), torch.arange(hi.shape[0])], dim=0)\n",
    "\n",
    "    # compute the InfoNCE loss\n",
    "    loss = F.cross_entropy(cos_sim / temperature, positive_pairs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why -1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Initialising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Initialising the Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimiser = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.183010244369507\n",
      "Epoch 1, Loss: 4.878868103027344\n",
      "Epoch 2, Loss: 4.79930546283722\n",
      "Epoch 3, Loss: 4.722259259223938\n",
      "Epoch 4, Loss: 4.674849963188171\n",
      "Epoch 5, Loss: 4.652444076538086\n",
      "Epoch 6, Loss: 4.630107831954956\n",
      "Epoch 7, Loss: 4.5965351343154905\n",
      "Epoch 8, Loss: 4.582862269878388\n",
      "Epoch 9, Loss: 4.572400486469268\n",
      "Epoch 10, Loss: 4.555621969699859\n",
      "Epoch 11, Loss: 4.552079820632935\n",
      "Epoch 12, Loss: 4.544319927692413\n",
      "Epoch 13, Loss: 4.540510141849518\n",
      "Epoch 14, Loss: 4.530709671974182\n",
      "Epoch 15, Loss: 4.5255736947059635\n",
      "Epoch 16, Loss: 4.522501242160797\n",
      "Epoch 17, Loss: 4.516883659362793\n",
      "Epoch 18, Loss: 4.512508749961853\n",
      "Epoch 19, Loss: 4.511414754390716\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    batch = 0\n",
    "    for (xi, xj), _ in trainloader:\n",
    "        batch += 1\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        hi, hj = model(xi), model(xj)\n",
    "        loss = contrastive_loss(hi, hj)\n",
    "\n",
    "        # loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # backward + optimise\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    # print statistics\n",
    "    print(f'Epoch {epoch}, Loss: {running_loss / batch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 With Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Loading the Pretrained Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloader, model):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xi, target in dataloader:\n",
    "            feature = model.forward_no_projection_head(xi)\n",
    "            features.append(feature)\n",
    "            labels.append(target)\n",
    "\n",
    "    return torch.cat(features), torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_transforms = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "linear_dataset = datasets.MNIST('./data', train=True, download=True, transform=linear_transforms)\n",
    "linear_trainloader = DataLoader(linear_dataset, batch_size=linear_batch_size, shuffle=True)\n",
    "\n",
    "features, labels = extract_features(linear_trainloader, model)\n",
    "linear_dataset = torch.utils.data.TensorDataset(features, labels)\n",
    "linear_trainloader = DataLoader(linear_dataset, batch_size=linear_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Initialising the Linear Classifier and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier = nn.Linear(4 * 4 * 50, 10)\n",
    "linear_optimiser = optim.Adam(linear_classifier.parameters(), lr=linear_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Linear Classification With Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] loss: 0.29376, accuracy: 94.793\n",
      "[Epoch: 2] loss: 0.17906, accuracy: 96.972\n",
      "[Epoch: 3] loss: 0.16473, accuracy: 97.290\n",
      "[Epoch: 4] loss: 0.16382, accuracy: 97.660\n",
      "[Epoch: 5] loss: 0.16249, accuracy: 97.905\n",
      "[Epoch: 6] loss: 0.16378, accuracy: 97.900\n",
      "[Epoch: 7] loss: 0.14519, accuracy: 98.200\n",
      "[Epoch: 8] loss: 0.13884, accuracy: 98.302\n",
      "[Epoch: 9] loss: 0.15767, accuracy: 98.210\n",
      "[Epoch: 10] loss: 0.13330, accuracy: 98.508\n",
      "[Epoch: 11] loss: 0.11651, accuracy: 98.630\n",
      "[Epoch: 12] loss: 0.12480, accuracy: 98.537\n",
      "[Epoch: 13] loss: 0.13676, accuracy: 98.512\n",
      "[Epoch: 14] loss: 0.11128, accuracy: 98.750\n",
      "[Epoch: 15] loss: 0.13256, accuracy: 98.632\n",
      "[Epoch: 16] loss: 0.12692, accuracy: 98.637\n",
      "[Epoch: 17] loss: 0.11972, accuracy: 98.822\n",
      "[Epoch: 18] loss: 0.13074, accuracy: 98.670\n",
      "[Epoch: 19] loss: 0.12536, accuracy: 98.763\n",
      "[Epoch: 20] loss: 0.09642, accuracy: 98.992\n",
      "[Epoch: 21] loss: 0.12821, accuracy: 98.773\n",
      "[Epoch: 22] loss: 0.10557, accuracy: 98.993\n",
      "[Epoch: 23] loss: 0.10855, accuracy: 98.953\n",
      "[Epoch: 24] loss: 0.12780, accuracy: 98.835\n",
      "[Epoch: 25] loss: 0.11185, accuracy: 98.902\n",
      "[Epoch: 26] loss: 0.10992, accuracy: 98.988\n",
      "[Epoch: 27] loss: 0.09944, accuracy: 99.093\n",
      "[Epoch: 28] loss: 0.08947, accuracy: 99.067\n",
      "[Epoch: 29] loss: 0.10331, accuracy: 99.035\n",
      "[Epoch: 30] loss: 0.08938, accuracy: 99.105\n",
      "[Epoch: 31] loss: 0.09223, accuracy: 99.083\n",
      "[Epoch: 32] loss: 0.08608, accuracy: 99.205\n",
      "[Epoch: 33] loss: 0.09425, accuracy: 99.128\n",
      "[Epoch: 34] loss: 0.07323, accuracy: 99.230\n",
      "[Epoch: 35] loss: 0.07340, accuracy: 99.250\n",
      "[Epoch: 36] loss: 0.09985, accuracy: 99.097\n",
      "[Epoch: 37] loss: 0.07316, accuracy: 99.292\n",
      "[Epoch: 38] loss: 0.09088, accuracy: 99.193\n",
      "[Epoch: 39] loss: 0.06961, accuracy: 99.267\n",
      "[Epoch: 40] loss: 0.07530, accuracy: 99.277\n",
      "[Epoch: 41] loss: 0.07968, accuracy: 99.255\n",
      "[Epoch: 42] loss: 0.07744, accuracy: 99.273\n",
      "[Epoch: 43] loss: 0.07058, accuracy: 99.318\n",
      "[Epoch: 44] loss: 0.07777, accuracy: 99.285\n",
      "[Epoch: 45] loss: 0.07314, accuracy: 99.323\n",
      "[Epoch: 46] loss: 0.07294, accuracy: 99.292\n",
      "[Epoch: 47] loss: 0.09084, accuracy: 99.237\n",
      "[Epoch: 48] loss: 0.06079, accuracy: 99.392\n",
      "[Epoch: 49] loss: 0.06409, accuracy: 99.435\n",
      "[Epoch: 50] loss: 0.08268, accuracy: 99.248\n",
      "[Epoch: 51] loss: 0.05526, accuracy: 99.478\n",
      "[Epoch: 52] loss: 0.06061, accuracy: 99.383\n",
      "[Epoch: 53] loss: 0.07894, accuracy: 99.337\n",
      "[Epoch: 54] loss: 0.06411, accuracy: 99.428\n",
      "[Epoch: 55] loss: 0.07124, accuracy: 99.360\n",
      "[Epoch: 56] loss: 0.05446, accuracy: 99.483\n",
      "[Epoch: 57] loss: 0.07979, accuracy: 99.322\n",
      "[Epoch: 58] loss: 0.06126, accuracy: 99.445\n",
      "[Epoch: 59] loss: 0.05389, accuracy: 99.512\n",
      "[Epoch: 60] loss: 0.05160, accuracy: 99.493\n",
      "[Epoch: 61] loss: 0.05614, accuracy: 99.507\n",
      "[Epoch: 62] loss: 0.04702, accuracy: 99.560\n",
      "[Epoch: 63] loss: 0.07952, accuracy: 99.332\n",
      "[Epoch: 64] loss: 0.07680, accuracy: 99.347\n",
      "[Epoch: 65] loss: 0.05395, accuracy: 99.522\n",
      "[Epoch: 66] loss: 0.06629, accuracy: 99.458\n",
      "[Epoch: 67] loss: 0.06244, accuracy: 99.463\n",
      "[Epoch: 68] loss: 0.03120, accuracy: 99.678\n",
      "[Epoch: 69] loss: 0.07396, accuracy: 99.382\n",
      "[Epoch: 70] loss: 0.06538, accuracy: 99.463\n",
      "[Epoch: 71] loss: 0.07332, accuracy: 99.418\n",
      "[Epoch: 72] loss: 0.06283, accuracy: 99.457\n",
      "[Epoch: 73] loss: 0.05045, accuracy: 99.540\n",
      "[Epoch: 74] loss: 0.04320, accuracy: 99.598\n",
      "[Epoch: 75] loss: 0.03992, accuracy: 99.632\n",
      "[Epoch: 76] loss: 0.04585, accuracy: 99.578\n",
      "[Epoch: 77] loss: 0.04902, accuracy: 99.560\n",
      "[Epoch: 78] loss: 0.05419, accuracy: 99.545\n",
      "[Epoch: 79] loss: 0.04753, accuracy: 99.585\n",
      "[Epoch: 80] loss: 0.05366, accuracy: 99.543\n",
      "[Epoch: 81] loss: 0.04453, accuracy: 99.600\n",
      "[Epoch: 82] loss: 0.05446, accuracy: 99.513\n",
      "[Epoch: 83] loss: 0.04422, accuracy: 99.608\n",
      "[Epoch: 84] loss: 0.03889, accuracy: 99.632\n",
      "[Epoch: 85] loss: 0.04877, accuracy: 99.550\n",
      "[Epoch: 86] loss: 0.05857, accuracy: 99.562\n",
      "[Epoch: 87] loss: 0.03772, accuracy: 99.655\n",
      "[Epoch: 88] loss: 0.05476, accuracy: 99.530\n",
      "[Epoch: 89] loss: 0.04191, accuracy: 99.647\n",
      "[Epoch: 90] loss: 0.04544, accuracy: 99.580\n",
      "[Epoch: 91] loss: 0.04042, accuracy: 99.638\n",
      "[Epoch: 92] loss: 0.04597, accuracy: 99.638\n",
      "[Epoch: 93] loss: 0.05532, accuracy: 99.532\n",
      "[Epoch: 94] loss: 0.03779, accuracy: 99.655\n",
      "[Epoch: 95] loss: 0.04136, accuracy: 99.663\n",
      "[Epoch: 96] loss: 0.03975, accuracy: 99.645\n",
      "[Epoch: 97] loss: 0.04949, accuracy: 99.615\n",
      "[Epoch: 98] loss: 0.04461, accuracy: 99.590\n",
      "[Epoch: 99] loss: 0.04102, accuracy: 99.645\n",
      "[Epoch: 100] loss: 0.03265, accuracy: 99.675\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch, data in enumerate(linear_trainloader, 0):\n",
    "        features, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        linear_optimiser.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        outputs = linear_classifier(features)\n",
    "\n",
    "        # loss\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "        # accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # optimise\n",
    "        linear_optimiser.step()\n",
    "\n",
    "    # print statistics\n",
    "    accuracy = correct / total * 100\n",
    "    print('[Epoch: %d] loss: %.5f, accuracy: %.3f' % (epoch + 1, running_loss / (batch+1), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Without Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dataset = datasets.MNIST('./data', train=True, download=True, transform=linear_transforms)\n",
    "linear_trainloader = DataLoader(linear_dataset, batch_size=linear_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Initialising the Linear Classifier and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier = nn.Linear(28*28, 10)\n",
    "linear_optimiser = optim.Adam(linear_classifier.parameters(), lr=linear_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Linear Classification Without Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] loss: 0.66095, accuracy: 82.643\n",
      "[Epoch: 2] loss: 0.36965, accuracy: 89.618\n",
      "[Epoch: 3] loss: 0.33349, accuracy: 90.492\n",
      "[Epoch: 4] loss: 0.31657, accuracy: 90.938\n",
      "[Epoch: 5] loss: 0.30648, accuracy: 91.222\n",
      "[Epoch: 6] loss: 0.29840, accuracy: 91.530\n",
      "[Epoch: 7] loss: 0.29253, accuracy: 91.792\n",
      "[Epoch: 8] loss: 0.29005, accuracy: 91.772\n",
      "[Epoch: 9] loss: 0.28635, accuracy: 91.918\n",
      "[Epoch: 10] loss: 0.28342, accuracy: 91.972\n",
      "[Epoch: 11] loss: 0.28029, accuracy: 92.168\n",
      "[Epoch: 12] loss: 0.27812, accuracy: 92.198\n",
      "[Epoch: 13] loss: 0.27644, accuracy: 92.307\n",
      "[Epoch: 14] loss: 0.27563, accuracy: 92.263\n",
      "[Epoch: 15] loss: 0.27143, accuracy: 92.362\n",
      "[Epoch: 16] loss: 0.27220, accuracy: 92.343\n",
      "[Epoch: 17] loss: 0.27021, accuracy: 92.478\n",
      "[Epoch: 18] loss: 0.27040, accuracy: 92.428\n",
      "[Epoch: 19] loss: 0.26772, accuracy: 92.538\n",
      "[Epoch: 20] loss: 0.26672, accuracy: 92.493\n",
      "[Epoch: 21] loss: 0.26613, accuracy: 92.520\n",
      "[Epoch: 22] loss: 0.26517, accuracy: 92.648\n",
      "[Epoch: 23] loss: 0.26526, accuracy: 92.522\n",
      "[Epoch: 24] loss: 0.26285, accuracy: 92.597\n",
      "[Epoch: 25] loss: 0.26282, accuracy: 92.650\n",
      "[Epoch: 26] loss: 0.26299, accuracy: 92.562\n",
      "[Epoch: 27] loss: 0.26206, accuracy: 92.662\n",
      "[Epoch: 28] loss: 0.26194, accuracy: 92.672\n",
      "[Epoch: 29] loss: 0.25938, accuracy: 92.815\n",
      "[Epoch: 30] loss: 0.26004, accuracy: 92.788\n",
      "[Epoch: 31] loss: 0.25956, accuracy: 92.730\n",
      "[Epoch: 32] loss: 0.25984, accuracy: 92.795\n",
      "[Epoch: 33] loss: 0.25862, accuracy: 92.772\n",
      "[Epoch: 34] loss: 0.25681, accuracy: 92.878\n",
      "[Epoch: 35] loss: 0.25773, accuracy: 92.745\n",
      "[Epoch: 36] loss: 0.25855, accuracy: 92.672\n",
      "[Epoch: 37] loss: 0.25607, accuracy: 92.878\n",
      "[Epoch: 38] loss: 0.25580, accuracy: 92.832\n",
      "[Epoch: 39] loss: 0.25539, accuracy: 92.897\n",
      "[Epoch: 40] loss: 0.25552, accuracy: 92.877\n",
      "[Epoch: 41] loss: 0.25512, accuracy: 92.890\n",
      "[Epoch: 42] loss: 0.25437, accuracy: 92.887\n",
      "[Epoch: 43] loss: 0.25458, accuracy: 92.942\n",
      "[Epoch: 44] loss: 0.25517, accuracy: 92.828\n",
      "[Epoch: 45] loss: 0.25341, accuracy: 92.920\n",
      "[Epoch: 46] loss: 0.25328, accuracy: 93.023\n",
      "[Epoch: 47] loss: 0.25314, accuracy: 92.900\n",
      "[Epoch: 48] loss: 0.25309, accuracy: 92.880\n",
      "[Epoch: 49] loss: 0.25146, accuracy: 93.032\n",
      "[Epoch: 50] loss: 0.25249, accuracy: 93.003\n",
      "[Epoch: 51] loss: 0.25112, accuracy: 93.035\n",
      "[Epoch: 52] loss: 0.25242, accuracy: 92.993\n",
      "[Epoch: 53] loss: 0.25102, accuracy: 92.977\n",
      "[Epoch: 54] loss: 0.25097, accuracy: 93.007\n",
      "[Epoch: 55] loss: 0.25143, accuracy: 92.875\n",
      "[Epoch: 56] loss: 0.25086, accuracy: 93.012\n",
      "[Epoch: 57] loss: 0.24994, accuracy: 93.000\n",
      "[Epoch: 58] loss: 0.24913, accuracy: 93.117\n",
      "[Epoch: 59] loss: 0.24857, accuracy: 93.088\n",
      "[Epoch: 60] loss: 0.25016, accuracy: 93.018\n",
      "[Epoch: 61] loss: 0.25083, accuracy: 93.018\n",
      "[Epoch: 62] loss: 0.24976, accuracy: 93.040\n",
      "[Epoch: 63] loss: 0.24820, accuracy: 93.088\n",
      "[Epoch: 64] loss: 0.24830, accuracy: 93.138\n",
      "[Epoch: 65] loss: 0.24853, accuracy: 93.132\n",
      "[Epoch: 66] loss: 0.24918, accuracy: 93.065\n",
      "[Epoch: 67] loss: 0.24911, accuracy: 93.055\n",
      "[Epoch: 68] loss: 0.24878, accuracy: 93.040\n",
      "[Epoch: 69] loss: 0.24640, accuracy: 93.147\n",
      "[Epoch: 70] loss: 0.24755, accuracy: 93.087\n",
      "[Epoch: 71] loss: 0.24741, accuracy: 93.133\n",
      "[Epoch: 72] loss: 0.24621, accuracy: 93.183\n",
      "[Epoch: 73] loss: 0.24640, accuracy: 93.168\n",
      "[Epoch: 74] loss: 0.24723, accuracy: 93.143\n",
      "[Epoch: 75] loss: 0.24916, accuracy: 92.928\n",
      "[Epoch: 76] loss: 0.24608, accuracy: 93.210\n",
      "[Epoch: 77] loss: 0.24580, accuracy: 93.142\n",
      "[Epoch: 78] loss: 0.24477, accuracy: 93.172\n",
      "[Epoch: 79] loss: 0.24589, accuracy: 93.150\n",
      "[Epoch: 80] loss: 0.24575, accuracy: 93.137\n",
      "[Epoch: 81] loss: 0.24652, accuracy: 93.115\n",
      "[Epoch: 82] loss: 0.24726, accuracy: 93.025\n",
      "[Epoch: 83] loss: 0.24654, accuracy: 93.055\n",
      "[Epoch: 84] loss: 0.24428, accuracy: 93.197\n",
      "[Epoch: 85] loss: 0.24505, accuracy: 93.177\n",
      "[Epoch: 86] loss: 0.24501, accuracy: 93.292\n",
      "[Epoch: 87] loss: 0.24419, accuracy: 93.092\n",
      "[Epoch: 88] loss: 0.24388, accuracy: 93.175\n",
      "[Epoch: 89] loss: 0.24486, accuracy: 93.168\n",
      "[Epoch: 90] loss: 0.24493, accuracy: 93.262\n",
      "[Epoch: 91] loss: 0.24442, accuracy: 93.133\n",
      "[Epoch: 92] loss: 0.24359, accuracy: 93.248\n",
      "[Epoch: 93] loss: 0.24339, accuracy: 93.240\n",
      "[Epoch: 94] loss: 0.24397, accuracy: 93.267\n",
      "[Epoch: 95] loss: 0.24296, accuracy: 93.323\n",
      "[Epoch: 96] loss: 0.24388, accuracy: 93.182\n",
      "[Epoch: 97] loss: 0.24414, accuracy: 93.165\n",
      "[Epoch: 98] loss: 0.24209, accuracy: 93.252\n",
      "[Epoch: 99] loss: 0.24406, accuracy: 93.225\n",
      "[Epoch: 100] loss: 0.24349, accuracy: 93.232\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch, data in enumerate(linear_trainloader, 0):\n",
    "        features, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        linear_optimiser.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        features = features.view(features.size(0), -1)\n",
    "        outputs = linear_classifier(features)\n",
    "\n",
    "        # loss\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "        # accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # optimise\n",
    "        linear_optimiser.step()\n",
    "\n",
    "    # print statistics\n",
    "    accuracy = correct / total * 100\n",
    "    print('[Epoch: %d] loss: %.5f, accuracy: %.3f' % (epoch + 1, running_loss / (batch+1), accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl_mnist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
